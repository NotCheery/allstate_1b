{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B3361R88ok_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive in Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFerNjmCnAvW",
        "outputId": "35ba46c8-7060-4a03-86c4-725da5db3343"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.inspection import permutation_importance\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "udYhnL4UnZY5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data (Select on folder in left pane and you should see your drive)\n",
        "filepath = \"/content/drive/MyDrive/claims_data (1).csv\"\n",
        "df = pd.read_csv(filepath)"
      ],
      "metadata": {
        "id": "uIqarx5kndZ7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying basic info about dataset\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(\"\\Column names:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nTarget variable distribution:\")\n",
        "print(df['loss'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0BqAvcZo5QG",
        "outputId": "5cb6926d-4dcf-4a3d-962a-b4aa0bdad63c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (188318, 132)\n",
            "\n",
            "First few rows:\n",
            "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9  ...     cont6     cont7  \\\n",
            "0   1    A    B    A    B    A    A    A    A    B  ...  0.718367  0.335060   \n",
            "1   2    A    B    A    A    A    A    A    A    B  ...  0.438917  0.436585   \n",
            "2   5    A    B    A    A    B    A    A    A    B  ...  0.289648  0.315545   \n",
            "3  10    B    B    A    B    A    A    A    A    B  ...  0.440945  0.391128   \n",
            "4  11    A    B    A    B    A    A    A    A    B  ...  0.178193  0.247408   \n",
            "\n",
            "     cont8    cont9   cont10    cont11    cont12    cont13    cont14     loss  \n",
            "0  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493  0.714843  2213.18  \n",
            "1  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431  0.304496  1283.60  \n",
            "2  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709  0.774425  3005.09  \n",
            "3  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077  0.602642   939.85  \n",
            "4  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011  0.432606  2763.85  \n",
            "\n",
            "[5 rows x 132 columns]\n",
            "\\Column names:\n",
            "['id', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18', 'cat19', 'cat20', 'cat21', 'cat22', 'cat23', 'cat24', 'cat25', 'cat26', 'cat27', 'cat28', 'cat29', 'cat30', 'cat31', 'cat32', 'cat33', 'cat34', 'cat35', 'cat36', 'cat37', 'cat38', 'cat39', 'cat40', 'cat41', 'cat42', 'cat43', 'cat44', 'cat45', 'cat46', 'cat47', 'cat48', 'cat49', 'cat50', 'cat51', 'cat52', 'cat53', 'cat54', 'cat55', 'cat56', 'cat57', 'cat58', 'cat59', 'cat60', 'cat61', 'cat62', 'cat63', 'cat64', 'cat65', 'cat66', 'cat67', 'cat68', 'cat69', 'cat70', 'cat71', 'cat72', 'cat73', 'cat74', 'cat75', 'cat76', 'cat77', 'cat78', 'cat79', 'cat80', 'cat81', 'cat82', 'cat83', 'cat84', 'cat85', 'cat86', 'cat87', 'cat88', 'cat89', 'cat90', 'cat91', 'cat92', 'cat93', 'cat94', 'cat95', 'cat96', 'cat97', 'cat98', 'cat99', 'cat100', 'cat101', 'cat102', 'cat103', 'cat104', 'cat105', 'cat106', 'cat107', 'cat108', 'cat109', 'cat110', 'cat111', 'cat112', 'cat113', 'cat114', 'cat115', 'cat116', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14', 'loss']\n",
            "\n",
            "Target variable distribution:\n",
            "count    188318.000000\n",
            "mean       3037.337686\n",
            "std        2904.086186\n",
            "min           0.670000\n",
            "25%        1204.460000\n",
            "50%        2115.570000\n",
            "75%        3864.045000\n",
            "max      121012.250000\n",
            "Name: loss, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "def preprocess_data(df):\n",
        "    data = df.copy()  # To avoid modifying original data\n",
        "\n",
        "    # Separating features and target\n",
        "    X = data.drop('loss', axis=1)\n",
        "    y = data['loss']\n",
        "\n",
        "    # Identify categorical and numerical columns\n",
        "    cat_cols = [col for col in X.columns if col.startswith('cat')]\n",
        "    cont_cols = [col for col in X.columns if col.startswith('cont')]\n",
        "\n",
        "    print(f\"Categorical columns: {len(cat_cols)}\")\n",
        "    print(f\"Continuous columns: {len(cont_cols)}\")\n",
        "\n",
        "    # Handling categorical variables by encoding\n",
        "    label_encoders = {}\n",
        "    for col in cat_cols:\n",
        "        le = LabelEncoder()\n",
        "        # Handling any new categories that might appear in test data by using astype(str)\n",
        "        X[col] = le.fit_transform(X[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    # Handling missing values - FIXED THIS LINE\n",
        "    X = X.fillna(X.mean())\n",
        "    for col in cat_cols:\n",
        "        X[col] = X[col].fillna(X[col].mode()[0]) if len(X[col].mode()) > 0 else 0\n",
        "\n",
        "    return X, y, cat_cols, cont_cols, label_encoders\n",
        "\n",
        "# Preprocessing data\n",
        "X, y, cat_cols, cont_cols, label_encoders = preprocess_data(df)\n",
        "\n",
        "print(f\"Final feature matrix shape: {X.shape}\")\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "\n",
        "# Scale continuous features - FIXED THIS LINE\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()  # FIXED: Removed asterisk\n",
        "\n",
        "# Scale only continuous features\n",
        "if cont_cols:\n",
        "    cont_indices = [X.columns.get_loc(col) for col in cont_cols]\n",
        "    X_train_scaled.iloc[:, cont_indices] = scaler.fit_transform(X_train.iloc[:, cont_indices])\n",
        "    X_test_scaled.iloc[:, cont_indices] = scaler.transform(X_test.iloc[:, cont_indices])\n",
        "\n",
        "# Basic Random Forest Model\n",
        "print(\"\\nTraining Basic Random Forest Model...\")\n",
        "rf_basic = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_basic.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_basic = rf_basic.predict(X_test)\n",
        "\n",
        "# Evaluate Basic model\n",
        "mse_basic = mean_squared_error(y_test, y_pred_basic)\n",
        "rmse_basic = np.sqrt(mse_basic)\n",
        "mae_basic = mean_absolute_error(y_test, y_pred_basic)\n",
        "r2_basic = r2_score(y_test, y_pred_basic)\n",
        "\n",
        "print(\"\\nBasic Random Forest Performance:\")\n",
        "print(f\"RMSE: {rmse_basic:.2f}\")\n",
        "print(f\"MAE: {mae_basic:.2f}\")\n",
        "print(f\"R^2 Score: {r2_basic:.4f}\")\n",
        "\n",
        "# Hyperparameter Tuning with GridSearchCV - FIXED: Added missing backslash\n",
        "print(\"\\nPerforming Hyperparameter Tuning...\")\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['auto', 'sqrt']\n",
        "}\n",
        "\n",
        "rf_tuned = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Use a smaller subset for faster grid search if dataset is large\n",
        "if len(X_train) > 1000:\n",
        "    X_train_sample, _, y_train_sample, _ = train_test_split(\n",
        "        X_train, y_train, train_size=1000, random_state=42\n",
        "    )\n",
        "else:\n",
        "    X_train_sample, y_train_sample = X_train, y_train\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    rf_tuned, param_grid, cv=3, scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1, verbose=1\n",
        ")\n",
        "grid_search.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Train final model with best parameters\n",
        "rf_final = grid_search.best_estimator_\n",
        "rf_final.fit(X_train, y_train)\n",
        "\n",
        "# Predictions with tuned model\n",
        "y_pred_final = rf_final.predict(X_test)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "mse_final = mean_squared_error(y_test, y_pred_final)\n",
        "rmse_final = np.sqrt(mse_final)\n",
        "mae_final = mean_absolute_error(y_test, y_pred_final)\n",
        "r2_final = r2_score(y_test, y_pred_final)\n",
        "\n",
        "print(\"\\nTuned Random Forest Performance:\")\n",
        "print(f\"RMSE: {rmse_final:.2f}\")\n",
        "print(f\"MAE: {mae_final:.2f}\")\n",
        "print(f\"R² Score: {r2_final:.4f}\")\n",
        "\n",
        "# Feature Importance Analysis\n",
        "print(\"\\nAnalyzing Feature Importance...\")\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': rf_final.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 20 Most Important Features:\")\n",
        "print(feature_importance.head(20))\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# 1. Feature Importance Plot\n",
        "plt.subplot(2, 2, 1)\n",
        "top_features = feature_importance.head(15)\n",
        "plt.barh(top_features['feature'], top_features['importance'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Top 15 Most Important Features')\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# 2. Actual vs Predicted Plot\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.scatter(y_test, y_pred_final, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual Loss')\n",
        "plt.ylabel('Predicted Loss')\n",
        "plt.title(f'Actual vs Predicted (R² = {r2_final:.3f})')\n",
        "\n",
        "# 3. Residual Plot\n",
        "plt.subplot(2, 2, 3)\n",
        "residuals = y_test - y_pred_final\n",
        "plt.scatter(y_pred_final, residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "\n",
        "# 4. Loss Distribution\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.hist(y, bins=50, alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Loss Amount')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Loss Amounts')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Cross-validation scores\n",
        "print(\"\\nPerforming Cross-Validation...\")\n",
        "cv_scores = cross_val_score(rf_final, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "cv_rmse_scores = np.sqrt(-cv_scores)\n",
        "\n",
        "print(f\"Cross-Validation RMSE scores: {cv_rmse_scores}\")\n",
        "print(f\"Mean CV RMSE: {cv_rmse_scores.mean():.2f} (+/- {cv_rmse_scores.std() * 2:.2f})\")\n",
        "\n",
        "# Permutation Importance (more robust feature importance)\n",
        "print(\"\\nCalculating Permutation Importance...\")\n",
        "perm_importance = permutation_importance(\n",
        "    rf_final, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
        ")\n",
        "\n",
        "perm_importance_df = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': perm_importance.importances_mean\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Features by Permutation Importance:\")\n",
        "print(perm_importance_df.head(10))\n",
        "\n",
        "# Analyze categorical vs continuous feature importance\n",
        "cat_importance = feature_importance[feature_importance['feature'].isin(cat_cols)]['importance'].sum()\n",
        "cont_importance = feature_importance[feature_importance['feature'].isin(cont_cols)]['importance'].sum()\n",
        "\n",
        "print(f\"\\nCategorical Features Total Importance: {cat_importance:.3f}\")\n",
        "print(f\"Continuous Features Total Importance: {cont_importance:.3f}\")\n",
        "\n",
        "# Model interpretation for business insights\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BUSINESS INSIGHTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Top categorical features\n",
        "top_cat_features = feature_importance[feature_importance['feature'].isin(cat_cols)].head(5)\n",
        "print(\"\\nTop 5 Most Important Categorical Features:\")\n",
        "for _, row in top_cat_features.iterrows():\n",
        "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "# Top continuous features\n",
        "top_cont_features = feature_importance[feature_importance['feature'].isin(cont_cols)].head(5)\n",
        "print(\"\\nTop 5 Most Important Continuous Features:\")\n",
        "for _, row in top_cont_features.iterrows():\n",
        "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "# Save important features to CSV\n",
        "feature_importance.to_csv('random_forest_feature_importance.csv', index=False)\n",
        "print(\"\\nFeature importance saved to 'random_forest_feature_importance.csv'\")\n",
        "\n",
        "# Model summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Final R² Score: {r2_final:.4f}\")\n",
        "print(f\"Final RMSE: {rmse_final:.2f}\")\n",
        "print(f\"Final MAE: {mae_final:.2f}\")\n",
        "print(f\"Number of features used: {len(X.columns)}\")\n",
        "print(f\"Number of trees in forest: {rf_final.n_estimators}\")\n",
        "\n",
        "# Save the trained model (optional)\n",
        "import joblib\n",
        "joblib.dump(rf_final, 'random_forest_claims_model.pkl')\n",
        "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
        "joblib.dump(scaler, 'feature_scaler.pkl')\n",
        "print(\"\\nModel and preprocessing objects saved to disk.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of9J90vBo-G-",
        "outputId": "032ac35e-a587-42d1-f99c-7968e583d46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical columns: 116\n",
            "Continuous columns: 14\n",
            "Final feature matrix shape: (188318, 131)\n",
            "Training set: (150654, 131)\n",
            "Test set: (37664, 131)\n",
            "\n",
            "Training Basic Random Forest Model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ETeFr3WKtY8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "b9GWTBhunDGo"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}