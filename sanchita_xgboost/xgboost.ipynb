{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c97d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      "Best Parameters Found:\n",
      "{'regressor__subsample': 0.8, 'regressor__n_estimators': 1000, 'regressor__min_child_weight': 3, 'regressor__max_depth': 4, 'regressor__learning_rate': 0.05, 'regressor__gamma': 0.5, 'regressor__colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import TargetEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "filename = os.path.join(os.getcwd(), \"data\", \"claims_data.csv\")\n",
    "df = pd.read_csv(filename)\n",
    "top_features = ['cat80', 'cat79', 'cat87', 'cat57', 'cat12', 'cat10', 'cat89', 'cat7', 'cat81', \n",
    "                'cat2', 'cat72', 'cat11', 'cat1', 'cat13', 'cat9', 'cat90', 'cat3', 'cat16', \n",
    "                'cat23', 'cat36', 'cat73', 'cat91', 'cat40', 'cat28', 'cat82', 'cat6', 'cat76',\n",
    "                'cat50', 'cat5', 'cat4', 'cat94', 'cat14', 'cat38', 'cat24', 'cat25', 'cat85',\n",
    "                'cat41', 'cat8', 'cat29', 'cat17', 'cat75', 'cat45', 'cat99', 'cat71', 'cat65', \n",
    "                'cat78', 'cat66', 'cat86', 'cat84', 'cat26']\n",
    "\n",
    "# Separate features and target\n",
    "X = df[top_features]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# 1. Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Log-transform target\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log  = np.log1p(y_test)\n",
    "\n",
    "# 3. Identify numeric & categorical columns\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# 4. Preprocessing\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', FunctionTransformer(np.log1p, validate=False), num_cols),\n",
    "    ('cat', TargetEncoder(), cat_cols)\n",
    "])\n",
    "\n",
    "# 5. Base XGBoost model\n",
    "xgb_base = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    tree_method='hist',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6. Combine preprocessing + model in a pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('regressor', xgb_base)\n",
    "])\n",
    "\n",
    "# 7. Expanded hyperparameter tuning\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [300, 500, 700, 1000],\n",
    "    'regressor__learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "    'regressor__max_depth': [3, 4, 6, 8, 10],\n",
    "    'regressor__subsample': [0.6, 0.8, 1.0],\n",
    "    'regressor__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'regressor__min_child_weight': [1, 3, 5, 7],\n",
    "    'regressor__gamma': [0, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipe,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,  # run 20 random combinations\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train_log)\n",
    "\n",
    "best_params = search.best_params_\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23e0b1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final RMSE: 2116.1671\n",
      "Final R²:   0.4512\n",
      "Final MAE:  1245.4641\n"
     ]
    }
   ],
   "source": [
    "# 8. Preprocess data for final training\n",
    "X_train_prep = search.best_estimator_.named_steps['preprocess'].transform(X_train)\n",
    "X_test_prep  = search.best_estimator_.named_steps['preprocess'].transform(X_test)\n",
    "\n",
    "# 9. Extract best hyperparameters and rebuild final model\n",
    "final_params = {\n",
    "    k.replace('regressor__',''): v\n",
    "    for k, v in best_params.items()\n",
    "    if k.replace('regressor__','') not in ['n_estimators', 'early_stopping_rounds']\n",
    "}\n",
    "\n",
    "# Add or override parameters for final training\n",
    "final_params.update({\n",
    "    'n_estimators': 5000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 1.0\n",
    "})\n",
    "\n",
    "# Initialize final XGBoost model\n",
    "final_model = XGBRegressor(**final_params)\n",
    "\n",
    "# 10. Fit final model with early stopping\n",
    "final_model.fit(\n",
    "    X_train_prep, y_train_log,\n",
    "    eval_set=[(X_test_prep, y_test_log)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 11. Predict & revert log-transform\n",
    "y_pred_log = final_model.predict(X_test_prep)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "# 12. Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nFinal RMSE: {rmse:.4f}\")\n",
    "print(f\"Final R²:   {r2:.4f}\")\n",
    "print(f\"Final MAE:  {mae:.4f}\")\n",
    "\n",
    "# 13. Plot early stopping curve\n",
    "evals_result = final_model.evals_result()\n",
    "if 'validation_0' in evals_result and 'validation_1' in evals_result:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(evals_result['validation_0']['rmse'], label='Train RMSE')\n",
    "    plt.plot(evals_result['validation_1']['rmse'], label='Test RMSE')\n",
    "    plt.xlabel('Boosting Round')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('Early Stopping Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
